{'paperId': '6caf3307096a15832ace34a0d54cd28413503f8b', 'url': 'https://www.semanticscholar.org/paper/6caf3307096a15832ace34a0d54cd28413503f8b', 'title': 'NeRF-: Neural Radiance Fields Without Known Camera Parameters', 'venue': 'arXiv.org', 'year': 2021, 'citationCount': 613, 'openAccessPdf': {'url': '', 'status': None, 'license': None}, 'authors': [{'authorId': '2117418332', 'name': 'Zirui Wang'}, {'authorId': '2146648', 'name': 'Shangzhe Wu'}, {'authorId': '10096695', 'name': 'Weidi Xie'}, {'authorId': '2145944904', 'name': 'Min Chen'}, {'authorId': '2824784', 'name': 'V. Prisacariu'}], 'abstract': None}
{'paperId': '5b0ea2c92ee16fa2f5a3dbc9315cd5c1e4ec1d88', 'url': 'https://www.semanticscholar.org/paper/5b0ea2c92ee16fa2f5a3dbc9315cd5c1e4ec1d88', 'title': 'NeRF++: Analyzing and Improving Neural Radiance Fields', 'venue': 'arXiv.org', 'year': 2020, 'citationCount': 1049, 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: Paper or abstract available at https://arxiv.org/abs/2010.07492, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '2158521200', 'name': 'Kai Zhang'}, {'authorId': '40310317', 'name': 'Gernot Riegler'}, {'authorId': '1830653', 'name': 'Noah Snavely'}, {'authorId': '145231047', 'name': 'V. Koltun'}], 'abstract': "Neural Radiance Fields (NeRF) achieve impressive view synthesis results for a variety of capture settings, including 360 capture of bounded scenes and forward-facing capture of bounded and unbounded scenes. NeRF fits multi-layer perceptrons (MLPs) representing view-invariant opacity and view-dependent color volumes to a set of training images, and samples novel views based on volume rendering techniques. In this technical report, we first remark on radiance fields and their potential ambiguities, namely the shape-radiance ambiguity, and analyze NeRF's success in avoiding such ambiguities. Second, we address a parametrization issue involved in applying NeRF to 360 captures of objects within large-scale, unbounded 3D scenes. Our method improves view synthesis fidelity in this challenging scenario. Code is available at this https URL."}
{'paperId': 'ae107f52af8c39faac1cfc219da13aa23f168e10', 'url': 'https://www.semanticscholar.org/paper/ae107f52af8c39faac1cfc219da13aa23f168e10', 'title': 'NeRF in Robotics: A Survey', 'venue': 'arXiv.org', 'year': 2024, 'citationCount': 15, 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: Paper or abstract available at https://arxiv.org/abs/2405.01333, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '2152583098', 'name': 'Guangming Wang'}, {'authorId': '2299780397', 'name': 'Lei Pan'}, {'authorId': '2267222520', 'name': 'Songyou Peng'}, {'authorId': '2156038785', 'name': 'Shaohui Liu'}, {'authorId': '1490695028', 'name': 'Chenfeng Xu'}, {'authorId': '2153966121', 'name': 'Yanzi Miao'}, {'authorId': '144267500', 'name': 'Wei Zhan'}, {'authorId': '2245825283', 'name': 'Masayoshi Tomizuka'}, {'authorId': '2263467446', 'name': 'Marc Pollefeys'}, {'authorId': '2241999604', 'name': 'Hesheng Wang'}], 'abstract': "Meticulous 3D environment representations have been a longstanding goal in computer vision and robotics fields. The recent emergence of neural implicit representations has introduced radical innovation to this field as implicit representations enable numerous capabilities. Among these, the Neural Radiance Field (NeRF) has sparked a trend because of the huge representational advantages, such as simplified mathematical models, compact environment storage, and continuous scene representations. Apart from computer vision, NeRF has also shown tremendous potential in the field of robotics. Thus, we create this survey to provide a comprehensive understanding of NeRF in the field of robotics. By exploring the advantages and limitations of NeRF, as well as its current applications and future potential, we hope to shed light on this promising area of research. Our survey is divided into two main sections: \\textit{The Application of NeRF in Robotics} and \\textit{The Advance of NeRF in Robotics}, from the perspective of how NeRF enters the field of robotics. In the first section, we introduce and analyze some works that have been or could be used in the field of robotics from the perception and interaction perspectives. In the second section, we show some works related to improving NeRF's own properties, which are essential for deploying NeRF in the field of robotics. In the discussion section of the review, we summarize the existing challenges and provide some valuable future research directions for reference."}
{'paperId': '2809a68abc9be8f1118ac2dea2af50e619b659c0', 'url': 'https://www.semanticscholar.org/paper/2809a68abc9be8f1118ac2dea2af50e619b659c0', 'title': 'DITTO-NeRF: Diffusion-based Iterative Text To Omni-directional 3D Model', 'venue': 'arXiv.org', 'year': 2023, 'citationCount': 43, 'openAccessPdf': {'url': 'http://arxiv.org/pdf/2304.02827', 'status': 'CLOSED', 'license': None, 'disclaimer': 'Notice: Paper or abstract available at https://arxiv.org/abs/2304.02827, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '51245215', 'name': 'H. Seo'}, {'authorId': '47298448', 'name': 'Hayeon Kim'}, {'authorId': '2109334279', 'name': 'Gwanghyun Kim'}, {'authorId': '34971370', 'name': 'S. Chun'}], 'abstract': 'The increasing demand for high-quality 3D content creation has motivated the development of automated methods for creating 3D object models from a single image and/or from a text prompt. However, the reconstructed 3D objects using state-of-the-art image-to-3D methods still exhibit low correspondence to the given image and low multi-view consistency. Recent state-of-the-art text-to-3D methods are also limited, yielding 3D samples with low diversity per prompt with long synthesis time. To address these challenges, we propose DITTO-NeRF, a novel pipeline to generate a high-quality 3D NeRF model from a text prompt or a single image. Our DITTO-NeRF consists of constructing high-quality partial 3D object for limited in-boundary (IB) angles using the given or text-generated 2D image from the frontal view and then iteratively reconstructing the remaining 3D NeRF using inpainting latent diffusion model. We propose progressive 3D object reconstruction schemes in terms of scales (low to high resolution), angles (IB angles initially to outer-boundary (OB) later), and masks (object to background boundary) in our DITTO-NeRF so that high-quality information on IB can be propagated into OB. Our DITTO-NeRF outperforms state-of-the-art methods in terms of fidelity and diversity qualitatively and quantitatively with much faster training times than prior arts on image/text-to-3D such as DreamFusion, and NeuralLift-360.'}
{'paperId': '04c36e581a4114096c07598eec310028206c4cc9', 'url': 'https://www.semanticscholar.org/paper/04c36e581a4114096c07598eec310028206c4cc9', 'title': 'OR-NeRF: Object Removing from 3D Scenes Guided by Multiview Segmentation with Neural Radiance Fields', 'venue': 'arXiv.org', 'year': 2023, 'citationCount': 30, 'openAccessPdf': {'url': 'https://arxiv.org/pdf/2305.10503', 'status': 'GREEN', 'license': None, 'disclaimer': 'Notice: Paper or abstract available at https://arxiv.org/abs/2305.10503, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '2034017386', 'name': 'Youtan Yin'}, {'authorId': '2157046216', 'name': 'Zhoujie Fu'}, {'authorId': '47829900', 'name': 'Fan Yang'}, {'authorId': '2604251', 'name': 'Guosheng Lin'}], 'abstract': 'The emergence of Neural Radiance Fields (NeRF) for novel view synthesis has increased interest in 3D scene editing. An essential task in editing is removing objects from a scene while ensuring visual reasonability and multiview consistency. However, current methods face challenges such as time-consuming object labeling, limited capability to remove specific targets, and compromised rendering quality after removal. This paper proposes a novel object-removing pipeline, named OR-NeRF, that can remove objects from 3D scenes with user-given points or text prompts on a single view, achieving better performance in less time than previous works. Our method spreads user annotations to all views through 3D geometry and sparse correspondence, ensuring 3D consistency with less processing burden. Then recent 2D segmentation model Segment-Anything (SAM) is applied to predict masks, and a 2D inpainting model is used to generate color supervision. Finally, our algorithm applies depth supervision and perceptual loss to maintain consistency in geometry and appearance after object removal. Experimental results demonstrate that our method achieves better editing quality with less time than previous works, considering both quality and quantity.'}
{'paperId': '811be823eca290d51eb751a6c5ef254a273a9614', 'url': 'https://www.semanticscholar.org/paper/811be823eca290d51eb751a6c5ef254a273a9614', 'title': 'ED-NeRF: Efficient Text-Guided Editing of 3D Scene using Latent Space NeRF', 'venue': 'arXiv.org', 'year': 2023, 'citationCount': 38, 'openAccessPdf': {'url': 'https://arxiv.org/pdf/2310.02712', 'status': 'CLOSED', 'license': None, 'disclaimer': 'Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2310.02712?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2310.02712, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '2254815571', 'name': 'Jangho Park'}, {'authorId': '116153377', 'name': 'Gihyun Kwon'}, {'authorId': '2254155689', 'name': 'Jong Chul Ye'}], 'abstract': 'Recently, there has been a significant advancement in text-to-image diffusion models, leading to groundbreaking performance in 2D image generation. These advancements have been extended to 3D models, enabling the generation of novel 3D objects from textual descriptions. This has evolved into NeRF editing meth-ods, which allow the manipulation of existing 3D objects through textual conditioning. However, existing NeRF editing techniques have faced limitations in their performance due to slow training speeds and the use of loss functions that do not adequately consider editing. To address this, here we present a novel 3D NeRF editing approach dubbed ED-NeRF by successfully embedding real-world scenes into the latent space of the latent diffusion model (LDM) through a unique refinement layer. This approach enables us to obtain a NeRF backbone that is not only faster but also more amenable to editing compared to traditional image space NeRF editing. Furthermore, we propose an improved loss function tailored for editing by migrating the delta denoising score (DDS) distillation loss, originally used in 2D image editing to the three-dimensional domain. This novel loss function surpasses the well-known score distillation sampling (SDS) loss in terms of suitability for editing purposes. Our experimental results demonstrate that ED-NeRF achieves faster editing speed while producing improved output quality compared to state-of-the-art 3D editing models.'}
{'paperId': 'cd23c49803ede0f71354d2ada153e0a9f76fa559', 'url': 'https://www.semanticscholar.org/paper/cd23c49803ede0f71354d2ada153e0a9f76fa559', 'title': 'NeRF: Neural Radiance Field in 3D Vision, A Comprehensive Review', 'venue': 'arXiv.org', 'year': 2022, 'citationCount': 194, 'openAccessPdf': {'url': 'http://arxiv.org/pdf/2210.00379', 'status': 'GREEN', 'license': None, 'disclaimer': 'Notice: Paper or abstract available at https://arxiv.org/abs/2210.00379, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '2135715429', 'name': 'K. Gao'}, {'authorId': '143741798', 'name': 'Yin Gao'}, {'authorId': '49372392', 'name': 'Hongjie He'}, {'authorId': '2186823480', 'name': 'Denning Lu'}, {'authorId': '2430221', 'name': 'Linlin Xu'}, {'authorId': '2109044237', 'name': 'Jonathan Li'}], 'abstract': 'In March 2020, Neural Radiance Field (NeRF) revolutionized Computer Vision, allowing for implicit, neural network-based scene representation and novel view synthesis. NeRF models have found diverse applications in robotics, urban mapping, autonomous navigation, virtual reality/augmented reality, and more. In August 2023, Gaussian Splatting, a direct competitor to the NeRF-based framework, was proposed, gaining tremendous momentum and overtaking NeRF-based research in terms of interest as the dominant framework for novel view synthesis. We present a comprehensive survey of NeRF papers from the past five years (2020-2025). These include papers from the pre-Gaussian Splatting era, where NeRF dominated the field for novel view synthesis and 3D implicit and hybrid representation neural field learning. We also include works from the post-Gaussian Splatting era where NeRF and implicit/hybrid neural fields found more niche applications. Our survey is organized into architecture and application-based taxonomies in the pre-Gaussian Splatting era, as well as a categorization of active research areas for NeRF, neural field, and implicit/hybrid neural representation methods. We provide an introduction to the theory of NeRF and its training via differentiable volume rendering. We also present a benchmark comparison of the performance and speed of classical NeRF, implicit and hybrid neural representation, and neural field models, and an overview of key datasets.'}
{'paperId': '3fc14981e43ed1617cb1316563d221f8ab448f5d', 'url': 'https://www.semanticscholar.org/paper/3fc14981e43ed1617cb1316563d221f8ab448f5d', 'title': 'CompoNeRF: Text-guided Multi-object Compositional NeRF with Editable 3D Scene Layout', 'venue': 'arXiv.org', 'year': 2023, 'citationCount': 45, 'openAccessPdf': {'url': 'https://arxiv.org/pdf/2303.13843', 'status': 'GREEN', 'license': None, 'disclaimer': 'Notice: Paper or abstract available at https://arxiv.org/abs/2303.13843, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '46396519', 'name': 'Yiqi Lin'}, {'authorId': '2153590470', 'name': 'Haotian Bai'}, {'authorId': '2128964829', 'name': 'Sijia Li'}, {'authorId': '2130373', 'name': 'H. Lu'}, {'authorId': '2117690698', 'name': 'Xiaodong Lin'}, {'authorId': '2054473562', 'name': 'Hui Xiong'}, {'authorId': '2168616303', 'name': 'Lin Wang'}], 'abstract': "Text-to-3D form plays a crucial role in creating editable 3D scenes for AR/VR. Recent advances have shown promise in merging neural radiance fields (NeRFs) with pre-trained diffusion models for text-to-3D object generation. However, one enduring challenge is their inadequate capability to accurately parse and regenerate consistent multi-object environments. Specifically, these models encounter difficulties in accurately representing quantity and style prompted by multi-object texts, often resulting in a collapse of the rendering fidelity that fails to match the semantic intricacies. Moreover, amalgamating these elements into a coherent 3D scene is a substantial challenge, stemming from generic distribution inherent in diffusion models. To tackle the issue of 'guidance collapse' and further enhance scene consistency, we propose a novel framework, dubbed CompoNeRF, by integrating an editable 3D scene layout with object-specific and scene-wide guidance mechanisms. It initiates by interpreting a complex text into the layout populated with multiple NeRFs, each paired with a corresponding subtext prompt for precise object depiction. Next, a tailored composition module seamlessly blends these NeRFs, promoting consistency, while the dual-level text guidance reduces ambiguity and boosts accuracy. Noticeably, our composition design permits decomposition. This enables flexible scene editing and recomposition into new scenes based on the edited layout or text prompts. Utilizing the open-source Stable Diffusion model, CompoNeRF generates multi-object scenes with high fidelity. Remarkably, our framework achieves up to a \\textbf{54\\%} improvement by the multi-view CLIP score metric. Our user study indicates that our method has significantly improved semantic accuracy, multi-view consistency, and individual recognizability for multi-object scene generation."}
{'paperId': 'c016164c9d7acae95a9aa59d1da3aadc40448ac2', 'url': 'https://www.semanticscholar.org/paper/c016164c9d7acae95a9aa59d1da3aadc40448ac2', 'title': 'NeRF-US: Removing Ultrasound Imaging Artifacts from Neural Radiance Fields in the Wild', 'venue': 'arXiv.org', 'year': 2024, 'citationCount': 6, 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: Paper or abstract available at https://arxiv.org/abs/2408.10258, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '2115011379', 'name': 'Rishit Dagli'}, {'authorId': '2192866444', 'name': 'Atsuhiro Hibi'}, {'authorId': '2066130378', 'name': 'R. Krishnan'}, {'authorId': '2281631331', 'name': 'Pascal N. Tyrrell'}], 'abstract': 'Current methods for performing 3D reconstruction and novel view synthesis (NVS) in ultrasound imaging data often face severe artifacts when training NeRF-based approaches. The artifacts produced by current approaches differ from NeRF floaters in general scenes because of the unique nature of ultrasound capture. Furthermore, existing models fail to produce reasonable 3D reconstructions when ultrasound data is captured or obtained casually in uncontrolled environments, which is common in clinical settings. Consequently, existing reconstruction and NVS methods struggle to handle ultrasound motion, fail to capture intricate details, and cannot model transparent and reflective surfaces. In this work, we introduced NeRF-US, which incorporates 3D-geometry guidance for border probability and scattering density into NeRF training, while also utilizing ultrasound-specific rendering over traditional volume rendering. These 3D priors are learned through a diffusion model. Through experiments conducted on our new"Ultrasound in the Wild"dataset, we observed accurate, clinically plausible, artifact-free reconstructions.'}
{'paperId': 'da2ef062c84d02e241a1531726ce8920f80af1da', 'url': 'https://www.semanticscholar.org/paper/da2ef062c84d02e241a1531726ce8920f80af1da', 'title': 'GauU-Scene V2: Assessing the Reliability of Image-Based Metrics with Expansive Lidar Image Dataset Using 3DGS and NeRF', 'venue': 'arXiv.org', 'year': 2024, 'citationCount': 5, 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: Paper or abstract available at https://arxiv.org/abs/2404.04880, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '2281036207', 'name': 'Butian Xiong'}, {'authorId': '2295669477', 'name': 'Nanjun Zheng'}, {'authorId': '2296736189', 'name': 'Junhua Liu'}, {'authorId': '2281078892', 'name': 'Zhen Li'}], 'abstract': 'We introduce a novel, multimodal large-scale scene reconstruction benchmark that utilizes newly developed 3D representation approaches: Gaussian Splatting and Neural Radiance Fields (NeRF). Our expansive U-Scene dataset surpasses any previously existing real large-scale outdoor LiDAR and image dataset in both area and point count. GauU-Scene encompasses over 6.5 square kilometers and features a comprehensive RGB dataset coupled with LiDAR ground truth. Additionally, we are the first to propose a LiDAR and image alignment method for a drone-based dataset. Our assessment of GauU-Scene includes a detailed analysis across various novel viewpoints, employing image-based metrics such as SSIM, LPIPS, and PSNR on NeRF and Gaussian Splatting based methods. This analysis reveals contradictory results when applying geometric-based metrics like Chamfer distance. The experimental results on our multimodal dataset highlight the unreliability of current image-based metrics and reveal significant drawbacks in geometric reconstruction using the current Gaussian Splatting-based method, further illustrating the necessity of our dataset for assessing geometry reconstruction tasks. We also provide detailed supplementary information on data collection protocols and make the dataset available on the following anonymous project page'}